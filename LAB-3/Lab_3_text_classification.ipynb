{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab-3-text-classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNZso77fp+4nG6fLuWyP2Zv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/masonreznov/CS-332-NLP-LAB/blob/main/LAB-3/Lab_3_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Text Classification of a movie review [dataset](https://www.cs.cornell.edu/people/pabo/movie-review-data/)** using machine learning classifiers.\n",
        "\n",
        "Steps/tasks: \n",
        "\n",
        "1.   Dataset collection\n",
        "2.   Mapping the training data and the labels \n",
        "3.   Data preprocessing\n",
        "4.   Vectorising/numerification of the text data\n",
        "5.   Training a classifier from standard library (sklearn)\n",
        "6.   Evaluating the performance in terms of accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "c9AfNN9EYcDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the necessary libraries\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.datasets import load_files\n",
        "nltk.download('stopwords')\n",
        "import pickle\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "5hgSpnfkcrIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be using the cornell movie review [dataset](https://www.cs.cornell.edu/people/pabo/movie-review-data/). The dataset contains 2000 documents and each of them are the reviews by an individual reviewer which is either positive or negative."
      ],
      "metadata": {
        "id": "PU-NqUj17KTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## This commands will download the dataset and extract in the current working directory\n",
        "\n",
        "!wget https://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\n",
        "!tar -xvzf review_polarity.tar.gz\n",
        "\n"
      ],
      "metadata": {
        "id": "XlNK_tQXm4Bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After extracting the dataset, `txt_sentence` folder will be\n",
        "created. This folder contains two sub-folders `neg` and `pos` where each of these categories have 1000 review documents.\n",
        "\n",
        "---\n",
        "The next step is to map each documents to their respective categories which can be accesed as per the need.\n",
        "\n",
        "As we have only two categories, so this can be facilited by traversing all the documents in the `neg` and `pos` folders and assign them as class 0 or 1 (0 maybe for `neg` and 1 for `pos` and vice-versa).\n",
        "\n",
        "\n",
        "You can also use the `load_files` method from [sklearn.datasets](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html) which has already been imported in cell #1."
      ],
      "metadata": {
        "id": "qoMCeBCL8T-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load_files take in the parent directory (just before the category subfolders) path as the arguwment\n",
        "# and can return the list of all training data (combined documents in a single list) as well as the respective categories \n",
        "\n",
        "# movie_data = load_files(\"txt_sentoken\", shuffle=False)\n",
        "movie_data = load_files(\"txt_sentoken\")"
      ],
      "metadata": {
        "id": "D8v4IPeLr9rA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = movie_data.data, movie_data.target\n",
        "\n",
        "\n",
        "# here X is the list of all text documents and y is a numpy array of all the categories\n",
        "# such that, the document number 1 i.e X[1] has the category y[1]  "
      ],
      "metadata": {
        "id": "QI5McnaAsSDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Total number of documents {len(X)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WLHqjYNswcK",
        "outputId": "aed423dd-2282-41cd-bd50-28178a6e000f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of documents 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Preprocessing the data\n",
        "## The labels are not required to be processed as they are already 0 and 1\n",
        "\n",
        "## Process the data X and store into the documents list\n",
        "\n",
        "\n",
        "### complete the following commented statements/blocks\n",
        "\n",
        "documents = []\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "stemmer = WordNetLemmatizer()\n",
        "\n",
        "for sen in range(0, len(X)):\n",
        "    ## Remove all the special characters or the non-word characters\n",
        "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
        "    \n",
        "    ## remove all single characters\n",
        "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "       \n",
        "    ## Substituting multiple spaces with single space\n",
        "    # document = \n",
        "        \n",
        "    ## Converting to Lowercase\n",
        "    # document =\n",
        "    \n",
        "    ### Lemmatization\n",
        "\n",
        "    ## split the document into words\n",
        "    # document = \n",
        "    ## use the stemmer.lemmatize() to lemmatize each words of the document \n",
        "    document = [stemmer.lemmatize(word) for word in document] \n",
        "    ## join each individual tokens of the document \n",
        "    # document = \n",
        "    \n",
        "    documents.append(document)"
      ],
      "metadata": {
        "id": "8XTYh6N-tErJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorizing the text in oder to change the text document into numerical values using [Bag of words](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
      ],
      "metadata": {
        "id": "2I5u6GuHICfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer(stop_words=stopwords.words('english'))\n",
        "X_bow = vectorizer.fit_transform(documents).toarray()"
      ],
      "metadata": {
        "id": "4377_XVzwT_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Splitting the training and testing instances into 80:20 ratio, 80% training and 20% testing\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_bow_train, X_bow_test, y_bow_train, y_bow_test = train_test_split(X_bow, y, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "TOy5J_f01TnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Classifying the `pos` and `neg` categories using Gasussian Naive bayes\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_bow_train, y_bow_train)"
      ],
      "metadata": {
        "id": "EBn1M8bT27OZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_bow_pred = gnb.predict(X_bow_test)\n",
        "## y_bow_pred is the predicted categories of the test data from learnt model \"gnb\""
      ],
      "metadata": {
        "id": "8PutyMmy3gON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(accuracy_score(y_bow_test, y_bow_pred))\n",
        "\n",
        "## accuracy measure in comparison of the predicted categories (y_bow_test) and the actual categories (y_bow_pred)"
      ],
      "metadata": {
        "id": "O5UHJeHR369w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tasks:\n",
        "Try with the following classifiers and report compare the accuracies with that of the `GaussianNaiveBayes`:\n",
        "\n",
        "1.   [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
        "\n",
        "2.   [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)\n",
        "\n",
        "----\n",
        "\n",
        "\n",
        "\n",
        "*   Repeat the same tasks i.e. classifying with the various classifiers but using [TFIDF](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) as the vectorizer instead of Bag of Words vectorizer and report the differences\n",
        "\n",
        "\n",
        "## For your reference, the above example is taken [from](https://stackabuse.com/text-classification-with-python-and-scikit-learn/)"
      ],
      "metadata": {
        "id": "DMFg4iALKwRa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Assignment**\n",
        "**Spoken language** classification task: Given a dataset consisting of a person name and their spoken language, train a classifier to predict the spoken language of a given unkown name. So, the person name is the data and the spoken language is your labels.\n",
        "\n",
        "## Datset [link](https://download.pytorch.org/tutorial/data.zip)\n",
        "\n",
        "## Dataset organisation\n",
        "    |-data\n",
        "    |    |-names\n",
        "    |          |-English \n",
        "    |          |-French\n",
        "    .          .\n",
        "    .          .\n",
        "    .          . \n",
        "    |          |-Japanese      \n",
        "    |-eng-fra.txt \n",
        "\n",
        "Here, each files inside the `names` folder is a collection of the person names according to the `language` and your first task is to map each of the names to the language in a datastructure which you can access. \n",
        "\n",
        "**Just ignore the `eng-fra.txt`** \n",
        "\n",
        "Finally, apply all the steps that were done in the movie review task to train classifiers for this language classification task.\n",
        "\n",
        "## NOTE:\n",
        "*  The assigment can be done either by a single person or in a group (max 10 in a group)\n",
        "*  Submit your assignmnent using this google [form](https://forms.gle/h3n5PoHtBiXDnN7p6).\n",
        "*  Submit on or before ~~next Tuesday (8/03/2022)~~ **23/03/2022**.\n",
        "*  The submission file should be a report in a pdf format which should include\n",
        "        1. The name and roll numbers of all the members\n",
        "        2. The objectives/goal of the experiments\n",
        "        3. Results (accuracy) should be reported in a tabular format comparing the effects of\n",
        "            1. choice of the vectoriser (Bag of words or TFIDF)\n",
        "            2. choice of the classifiers\n",
        "            3. explore whether the use of lemmatization improves the model or not.\n",
        "        4. Finally, conclude with your observation\n",
        "        5. Also, include your code as a colab link in the pdf itself.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CQRUJl7-Oliz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "r6CQfEz46Vwm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}