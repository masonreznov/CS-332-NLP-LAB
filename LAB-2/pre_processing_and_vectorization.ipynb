{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pre-processing_and_vectorization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNw6yWImImAYKz4Uh/l63A6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/masonreznov/CS-332-NLP-LAB/blob/main/LAB-2/pre_processing_and_vectorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1) Write a function to clean a given document. The Function should be able to \n",
        "1. normalise any non-ascii charaters\n",
        "2. lowercase every words\n",
        "3. perform the stemming\n",
        "4. removal of stopwords using nltk\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4tLTqowavYdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The first parameter of this fucntion is the document\n",
        "# Apart from this, the function should take 4 boolean flags as parameter  i.e normalise, lowercase, stemming, remove_stopwords\n",
        "# which should allow to use either all of these features or just a few.\n",
        "# The processing should be done in a sequence order of normalise -> lowercase -> stemming -> remove_stopwords\n",
        "\n",
        "# The function should return the clean text as a document. If you are using lists to store the intermediate texts\n",
        "# then use join() to merge all the list elements as a single document \n",
        "\n",
        "# You might need nltk.download('stopwords') \n",
        "## reference for the stopword usage: https://pythonspot.com/nltk-stop-words/\n",
        "\n",
        "document = \"\"\"Pizza (Italian: [ˈpittsa], Neapolitan: [ˈpittsə]) is a dish of Italian origin consisting of a usually round, flat base of leavened wheat-based dough topped with tomatoes, cheese, and often various other ingredients (such as anchovies, mushrooms, onions, olives, pineapple, meat, etc.), which is then baked at a high temperature, traditionally in a wood-fired oven.[1] A small pizza is sometimes called a pizzetta. A person who makes pizza is known as a pizzaiolo.\n",
        "\n",
        "In Italy, pizza served in a restaurant is presented unsliced, and is eaten with the use of a knife and fork. In casual settings, however, it is cut into wedges to be eaten while held in the hand.\n",
        "The term pizza was first recorded in the 10th century in a Latin manuscript from the Southern Italian town of Gaeta in Lazio, on the border with Campania. Modern pizza was invented in Naples, and the dish and its variants have since become popular in many countries.[5] It has become one of the most popular foods in the world and a common fast food item in Europe, North America and Australasia; available at pizzerias (restaurants specializing in pizza), restaurants offering Mediterranean cuisine, via pizza delivery, and as street food.[5] Various food companies sell ready-baked pizzas, which may be frozen, in grocery stores, to be reheated in a home oven.\n",
        "In 2017, the world pizza market was US$128 billion, and in the US it was $44 billion spread over 76,000 pizzerias. Overall, 13% of the U.S. population aged 2 years and over consumed pizza on any given day.\n",
        "\"\"\"\n",
        "def clean_document(document, normalise=True, lowercase=True, stemming=True, remove_stopwords=True):\n",
        "    \n",
        "    ### Begin your code\n",
        "\n",
        "    ### return the clean document\n",
        "    pass # remove this line after completing your code"
      ],
      "metadata": {
        "id": "dJMULPB7P8dQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2) Write a function to count the occurences of each words in a document."
      ],
      "metadata": {
        "id": "cbug6r91ynLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not use any external libraries\n",
        "# The function should take the document (either the cleaned or the raw) as a function parameter\n",
        "# And returns two values\n",
        "#    1) a dictionary containing the unique words as the dictionary key with their count as the value\n",
        "#    2) the list of unique words or the vocabulary\n",
        "\n",
        "\n",
        "def word_count(document):\n",
        "\n",
        "    ### Begin your code\n",
        "\n",
        "\n",
        "    # return word_count_dict, vocab ## returns the word count dictionary and the vocabulary\n",
        "    \n",
        "    pass # remove this line after completing your code"
      ],
      "metadata": {
        "id": "SH6nPwgVywP2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## CHECK the differences in the word counts and the vocab before and after cleaning the document\n",
        "## Uncomment the lines below and execute\n",
        "\n",
        "# _ ,vocab_raw_doc = word_count(document) \n",
        "# _ ,vocab_clean_doc = word_count(clean_document(document)) \n",
        "\n",
        "# print(f\"The vocabs before cleaning is {vocab_size_raw_doc}\"\")\n",
        "# print(\"-----------------------------------------------------------\")\n",
        "# print(f\"The vocabs before cleaning is {vocab_size_clean_doc}\")\n"
      ],
      "metadata": {
        "id": "sCYV2was0rmv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3) Extract the first sentence which is common in all the documents along with their index.\n",
        "\n",
        "## NOTE: The sentences in the documents should be splitted on the punctuations ., ! and ?\n",
        "\n",
        "* You might also need to lowercase the texts in order to match the uneven cases (upper and lower case)\n",
        "* Use your clean_document() to lowercase the text while turning off all the other feature flags of the clean_document function\n",
        "* Be carefull to not remove the punctuations before segmenting the sentences\n",
        "* You can also check the effect of stemming by enabling the stemming flag in the clean_document().\n",
        "\n",
        "Example 1:\n",
        "\n",
        "```\n",
        "    doc1 = \"\"\"Abba dabba.Bubba bubba?wuba Luba Dub Dub!\"\"\"\n",
        "    doc2 = \"\"\"Bubba bubbaasda?Abba DaBba.Wuba Luba Dub Dub Nighg!\"\"\"\n",
        "    doc3 = \"\"\"Bubba bubbaasda?abbA dabba.Wuba Luba Dub Dub Nighg!\"\"\"\n",
        "\n",
        "    output: 1 2 2 abba dabba\n",
        "\n",
        "    Here, Abba dabba ocurrs at the 1st, 2nd and 2nd position of the three documents respectively. \n",
        "```\n",
        "Example 2:\n",
        "```\n",
        "    doc1 = \"\"\"abba dabba.Bubba bubba?Wuba Luba Dub Dub!\"\"\"\n",
        "    doc2 = \"\"\"Bubba bubbaasda?ABBa dabba.Abba dabba.Wuba Luba Dub Dub Nighg!\"\"\"\n",
        "    doc3 = \"\"\"Bubba bubbaasda?ABBA DabbA.Wuba Luba Dub Dub Nighg!\"\"\"\n",
        "\n",
        "    output: 1 2 2 Abba dabba\n",
        "\n",
        "    Here, the first Abba dabba in doc2 occurs at 2nd position\n",
        "```\n",
        "Example 3:\n",
        "```\n",
        "    doc1 = \"\"\"Abba dabba.Bubba bubba?Wuba Luba Dub Dub!\"\"\"\n",
        "    doc2 = \"\"\"Bubba bubbaasda?Abba dabba.Wuba Luba Dub Dub Nighg!\"\"\"\n",
        "    doc3 = \"\"\"Bubba bubbaasda?Wuba Luba Dub Dub Nighg!\"\"\"\n",
        "\n",
        "    output: Not found in doc3!!!\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "wGgQjutE51at"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4) Given three sentences sen1, sen2 and sen3 do the following\n",
        "\n",
        "\n",
        "1.   Clean the sentences using the clean_document(). Disable the stemming flag\n",
        "2.   Merge the texts of all the sentences and extract the vocab using word_count() method.\n",
        "3.   For each sentences count the occurences of each vocabs\n",
        "\n",
        "Example: \n",
        "```\n",
        "sen1 = \"Cat saw a cat\"\n",
        "sen2 = \"I love huskies\"\n",
        "sen3 = \"cat meows\"\n",
        "\n",
        "Here the vocab will be [cat, saw, a, i, love, huskies, meows]\n",
        "\n",
        "Output:\n",
        "            cat     saw     a     i     love      huskies    meows\n",
        "    sen1    2       1       1     0     0         0          0\n",
        "    sen2    0       0       0     1     1         1          0\n",
        "    sen3    1       0       0     0     0         0          1 \n",
        "\n",
        "here, *cat* occurs 2 times, *saw* occurs 1 times, *a* occurs 1 times and the rest vocab occurs 0 times in the sen1 and so on.\n",
        "```\n",
        "The above method is [Bag of Words](https://machinelearningmastery.com/gentle-introduction-bag-words-model/) approach. Useful for vectorizing texts. \n",
        "\n",
        "*sen1, sen2 and sen3* in the above example can be represented as\n",
        "```\n",
        "sen1 = [2,1,1,0,0,0,0]\n",
        "sen2 = [0,0,0,1,1,1,0]\n",
        "sen3 = [1,0,0,0,0,0,1]\n",
        "```\n",
        "\n",
        "        \n",
        "\n"
      ],
      "metadata": {
        "id": "ri57-fRe-jqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using sklearn to vectorise the documents using Bag of words\n",
        "\n",
        "See [1](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) [2](https://towardsai.net/p/nlp/natural-language-processing-nlp-with-python-tutorial-for-beginners-1f54e610a1a0) for the reference.\n"
      ],
      "metadata": {
        "id": "c9AfNN9EYcDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the CountVectorizer module from sklearn\n",
        "# Change the documents to list if needed\n",
        "# Play the number of grams (n-gram) using the ngram_range=(_,_) arguement in the Countvectorizer\n",
        "# Compare the resultant count matrix with the one implemented using scratch\n",
        "## NOTE that the one implemented from the scratch is a 1-gram model.\n",
        "\n"
      ],
      "metadata": {
        "id": "RamB_vLAYX6b"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5hgSpnfkcrIj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
