{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab-4-word-embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMAmZPc3JRDrgL4+04l8FcD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/masonreznov/CS-332-NLP-LAB/blob/main/LAB-4/Lab_4_word_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################\n",
        "############ Code cell #1 ###########\n",
        "#####################################\n",
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import gensim.downloader as api"
      ],
      "metadata": {
        "id": "b7YZVMIC2N5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word embeddings with word2vec model using Gensim library/api\n",
        "\n",
        "\n",
        "1.   Firstly, using a pretrained word2vec model trained on google news.  \n",
        "2.   Each word is represented by a vector of size 300 in this model.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "x3VF32ZPmNzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################\n",
        "############ Code cell #2 ###########\n",
        "#####################################\n",
        "# Dowloading the pretrained word2vec model\n",
        "word2vec_model = api.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "id": "47GheuHF4c1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################\n",
        "############ Code cell #3 ###########\n",
        "#####################################\n",
        "# Vector representation of the word beautiful using this word2vec model\n",
        "print(f'The vector reperesentation of the word beautiful: {word2vec_model[\"beautiful\"]}')\n",
        "\n",
        "# The vector is stored as a numpy array of dimension (300,)\n",
        "print(\"----------------------------\")\n",
        "print(f'The length of the word vector is: {word2vec_model[\"beautiful\"].shape}')"
      ],
      "metadata": {
        "id": "iYmHia7U4fvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1:** Find the 6 most similar words to `panda`.\n",
        "**Hint** Reference [1](https://radimrehurek.com/gensim/models/word2vec.html) [2](https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.most_similar.html). Also, consider using the `topn` argument. "
      ],
      "metadata": {
        "id": "wy3jHq9ZtA2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################\n",
        "############ Code cell #4 ###########\n",
        "#####################################\n",
        "# use the word2vec_model object to do the task\n",
        "# Approx number of lines of code = 1\n",
        "# Your code here\n"
      ],
      "metadata": {
        "id": "k4dWnVhu7_9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2:** Analogy task\n",
        "\n",
        "Try some analogy task such as `king - man + woman = queen`\n",
        "\n",
        "**Hint** Refer [this](https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.most_similar.html). Here, only the highest similar word should be returned, hence set the value of `topn` argument accordingly. \n"
      ],
      "metadata": {
        "id": "jqXvwyxeugvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################\n",
        "############ Code cell #5 ###########\n",
        "#####################################\n",
        "# Approx number of lines of code = 1\n",
        "# Your code here"
      ],
      "metadata": {
        "id": "vsSyPrJ38R35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3:** Check the distance between the antonyms and the synonyms.\n",
        "\n",
        "Note: The more the words are similar in the vector space, their distance will be lesser\n"
      ],
      "metadata": {
        "id": "VOCOm-KRxbqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################\n",
        "############ Code cell #6 ###########\n",
        "#####################################\n",
        "print(word2vec_model.distance(\"happy\", \"glad\"))\n",
        "print(word2vec_model.distance(\"good\", \"great\"))\n",
        "print(word2vec_model.distance(\"happy\", \"sad\"))\n"
      ],
      "metadata": {
        "id": "WB6YjEHF8piD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 4:** Write a module to plot the word vectors in a 2-D vector space\n",
        "\n",
        "\n",
        "*   You need to reduce the 300 vector size (dimension) to a 2-D vector using [TSNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html?highlight=tsne#sklearn.manifold.TSNE).\n",
        "*   Each word is now represented by this approximated 2-D vector which you can consider as a point of 2 coordinates, where the first vector element is the x-coordinate and the second vector element is the y-coordinate. Finally, you can plot a scatter plot using your prefferred plotting libary. \n",
        "\n"
      ],
      "metadata": {
        "id": "i4bLEMhG7RNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################\n",
        "############ Code cell #7 ###########\n",
        "#####################################\n",
        "# we a list of words and extract their corresponding vectors from the word2vec model \n",
        "vocab = [\"boy\", \"girl\", \"man\", \"woman\", \"king\", \"queen\", \"banana\", \"apple\", \"mango\", \"fruit\", \"coconut\", \"orange\"]\n",
        "\n",
        "# the function takes the model as the argument and plots the scatter plot graph\n",
        "def tsne_plot(model):\n",
        "    labels = [] # list to append the labels, labels is the word in the vocab list\n",
        "    wordvecs = [] # list to append the extracted word vectors for the respective labels/words\n",
        "\n",
        "    ## complete this loop\n",
        "    for word in vocab: # iterate over the vocabs\n",
        "        ## Todo #1 : append each word vectors of the current word into the wordvecs list\n",
        "        ## Todo #2 : append each labels or the current word into the labels list\n",
        "        ## HinT: you can access the word vectors using ``word2vec_model[word]`` as done in ``Code cell #3``\n",
        "\n",
        "    # Reducing the word vectors from 300 to 2 dimensioality for visualizing in a 2-D space using TSNE\n",
        "    tsne_model = TSNE(perplexity=3, n_components=2, init='pca', random_state=42)\n",
        "    # complete the following to fit and transform the word vectors and store into the coordinates variable\n",
        "    coordinates = \n",
        "    x =  # initialise an empty list to store the x-coordinate from the coordinates variable\n",
        "    y =  # initialise an empty list to store the y-coordinate from the coordinates variable\n",
        "    \n",
        "    # complete the following loop to append each x and y values of the coordinates so that they can be used for the scatter plot.\n",
        "\n",
        "    for value in coordinates:\n",
        "        x.append() ## the first element of the coordinate value should go into the list of x-coordinate values\n",
        "        y.append() ## the second element of the coordinate value should go into the list of y-coordinate values\n",
        "        \n",
        "    plt.figure(figsize=(8,8)) \n",
        "    for i in range(len(x)):\n",
        "        plt.scatter(x[i],y[i])\n",
        "        plt.annotate(labels[i],\n",
        "                     xy=(x[i], y[i]),\n",
        "                     xytext=(2, 2),\n",
        "                     textcoords='offset points',\n",
        "                     )\n",
        "    plt.show()\n",
        "\n",
        "tsne_plot(word2vec_model)"
      ],
      "metadata": {
        "id": "NZh5tjr7872z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training your own word2vec model"
      ],
      "metadata": {
        "id": "1m5bznioD-o6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using 20newsgroups dataset from sklearn\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
        "documents = dataset.data\n",
        "\n"
      ],
      "metadata": {
        "id": "MJH-0lRMEHP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize the documents\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "token_list = []\n",
        "for d in documents:\n",
        "    s = sent_tokenize(d) \n",
        "    token_list = token_list +[word_tokenize(t) for t in s]\n",
        "\n",
        "# check the first three sentences\n",
        "token_list[:3]"
      ],
      "metadata": {
        "id": "0yFf1rEQENE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the custom word2vec model\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "# Number of vector elements (dimensions) to represent the word vector\n",
        "num_features = 50\n",
        "\n",
        "\n",
        "model = Word2Vec(token_list, size=num_features)"
      ],
      "metadata": {
        "id": "okZtMvkoEhEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dicarding unneccesary weights\n",
        "model.init_sims(replace=True)"
      ],
      "metadata": {
        "id": "uKjqH-EaE8pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the trained model for later use\n",
        "model_name = \"my_custom_word2vec_50\"\n",
        "model.save(model_name)"
      ],
      "metadata": {
        "id": "t50rtvNiGHUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# resuing the trained model\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "model_name = \"my_custom_word2vec_50\"\n",
        "custom_word2vec_model = Word2Vec.load(model_name) "
      ],
      "metadata": {
        "id": "h5nxZ7Y0GTaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('house')"
      ],
      "metadata": {
        "id": "ozMIdrJTIBRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **TASK 5** \n",
        "**Repeat the same tasks from Task 1 till Task 4 using**\n",
        "1.   Your custom trained word2vec model\n",
        "2.   Using a pretrained glove vector model such as `glove-wiki-gigaword-100`. You can see all the supported models using:  \n",
        "\n",
        "```\n",
        "import gensim.downloader\n",
        "print(list(gensim.downloader.info()['models'].keys()))\n",
        "```\n",
        "Refer [this](https://radimrehurek.com/gensim/models/word2vec.html) for more details.\n",
        "\n",
        "\n",
        "\n",
        "This notebook is referred [from](https://github.com/murthyrudra/IIITL_NLP_Lab/blob/main/Lab06/lab05-1.ipynb). Try not to directly copy paste before trying out yourself. However, you are free to refer to the official documentations whenever required.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ESrtKY05J5js"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **TASK 6**\n",
        "\n",
        "1.  Use the movie review [data]([dataset](https://www.cs.cornell.edu/people/pabo/movie-review-data/) to train a review classifier as done in [LAB-3](\n",
        "https://github.com/masonreznov/CS-332-NLP-LAB/blob/main/LAB-3/Lab_3_text_classification.ipynb) using word2vec word vectors instead of BAG of words and TFIDF.\n",
        "2. Train a custom word2vec model using the text documents of the review data\n",
        "3. As word2vec is a vector representation of a single word, so in-order to represent each review document which in turn is made up of sentences, you might have to combine all the word vectors for each sentences and take the average for each of each sentence vectors for the final document vector.\n",
        "4. Refer the `code cell 15` from this [link](https://github.com/gabrielwong159/movie-review/blob/master/3_word2vec.ipynb) for vectorizing each review documents.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "## SUBMIT your implementations as a colab notebook on or before 12/04/2022 in [link](https://forms.gle/YQmnZBSJPTYWk5VW7)\n"
      ],
      "metadata": {
        "id": "OU-tQh0UpKYa"
      }
    }
  ]
}